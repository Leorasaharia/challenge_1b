{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCtQq52-kBCA",
    "outputId": "29fec8e7-8c3b-4027-c0d2-a1ca7364ff28"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install --quiet transformers datasets accelerate torch sentencepiece PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "1_hfO_6ZkEzL",
    "outputId": "4a28e7cf-5f89-401a-f5af-5f8661e11b73"
   },
   "outputs": [],
   "source": [
    "# Cell 2A: Upload Collection 1 (Travel Planning)\n",
    "from google.colab import files\n",
    "print(\"▶️ Select all 7 PDFs\")\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "ve1bccbnkJ4p",
    "outputId": "54517a0a-905c-47b4-c146-89c12c1be9eb"
   },
   "outputs": [],
   "source": [
    "# Cell 2B: Upload Collection 2 (Adobe Acrobat Learning)\n",
    "from google.colab import files\n",
    "print(\"▶️ Select all 15 PDFs\")\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "MxpYjx7fkQTj",
    "outputId": "9553ba32-3d45-4e5d-fa6d-406cee3fa92e"
   },
   "outputs": [],
   "source": [
    "# Cell 2C: Upload Collection 3 (Recipe Collection)\n",
    "from google.colab import files\n",
    "print(\"▶️ Select all 9 PDFs\")\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgZ8agZukwca"
   },
   "outputs": [],
   "source": [
    "# Install PyPDF2 if not already installed\n",
    "!pip install --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEDPrH9ZkZnE",
    "outputId": "7913401a-d853-4712-bf6b-64ef9c717771"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Build train.jsonl from your 3 input/output pairs + PDFs\n",
    "import glob, json\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "examples = []\n",
    "for inp_fn in sorted(glob.glob(\"challenge1b_input*.json\")):\n",
    "    out_fn = inp_fn.replace(\"input\",\"output\")\n",
    "    inp, out = json.load(open(inp_fn)), json.load(open(out_fn))\n",
    "    # concatenate all pages\n",
    "    pages = []\n",
    "    for d in inp[\"documents\"]:\n",
    "        reader = PdfReader(d[\"filename\"])\n",
    "        for i,page in enumerate(reader.pages,1):\n",
    "            txt = page.extract_text() or \"\"\n",
    "            pages.append(f\"[{d['filename']} – PAGE {i}]\\n{txt}\")\n",
    "    inp_str = (\n",
    "        f\"Persona: {inp['persona']['role']}\\n\"\n",
    "        f\"Job: {inp['job_to_be_done']['task']}\\n\\n\"\n",
    "        + \"\\n\\n\".join(pages)\n",
    "    )\n",
    "    out_str = json.dumps(out[\"extracted_sections\"], ensure_ascii=False)\n",
    "    examples.append({\"input\": inp_str, \"output\": out_str})\n",
    "\n",
    "with open(\"train.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"✅ Wrote train.jsonl with {len(examples)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306,
     "referenced_widgets": [
      "5a33f5c516ef4cd79ef68892e9300d9c",
      "7bbedb8bbb354eafb4abd0fddc4947f7",
      "4cd47dc0a7a24c418d28ab89c0a9557a",
      "162b3351dc1346028d0ce0f4240d3c11",
      "f439abed60374748b8e84ecd4ad16790",
      "3c799b6ddc4c4534a453b4d2c4a37e67",
      "bea391f3859d48e09e6f0cff1a3cadef",
      "6584d2206f5d43d5a9ae6ba83bf8ec9c",
      "46ac11c47f2e45ad95ff67151deed577",
      "92f5bc2370804314b35985dd491e9b73",
      "376931d08d29458f8828aff6846587ae",
      "a528d219850b4be1bbeda605b1fe05b1",
      "e641c42ee871487f9edeec7629cbaa53",
      "43e1ef4e54d342ea8197eb492eb72944",
      "ac3404cb77b44e29b159f64798c3c5f6",
      "7bdb5b7e3edf4733b578806a20e357e3",
      "a897c667830b4eabab30215e4fc5054f",
      "bb1036c1a68942568a88ffa3530de025",
      "03396896f5b747d8bf882077ab0d85da",
      "fc24d3833f3b4944a231ff498625d83b",
      "26c86779e485467d8a638736ac43548b",
      "0b8d328caa04422298b76fbb32ab5c82",
      "a8fdb8c50f3d4419b186c2d9cf2f2502",
      "7145147bcfe147eeae9c1ff2dc84f056",
      "44026e114f7a48c19f0cb49b51c9c54b",
      "c0a28aa9310c4383ab1891f4ec4767a9",
      "f0716d039cd74c9f952cf865f376f215",
      "374ba72b66a642bcbeb094cce7f3c676",
      "1ad742e609fb43f29aa362afcd66dc4e",
      "cf986d79d9fa4e55bb98c7c09de88500",
      "99958eaaf0314be597315f0785a3364d",
      "5cc2559342734790a332df605dc9a745",
      "3ea86a957c5d40f4b1c07d372d2af539",
      "b87659fbfa1149438138d2c8dc85b418",
      "2efb658248e6428e9804e4329a86e5c7",
      "3ef959ec1ab743cd8a0454cd0a11b357",
      "3d1419404f8c40deb6576dae737aabc3",
      "5a521a6c0ce94911845ec3b39f047323",
      "bac35b2de3de47229896b87330e733f3",
      "40c7ba7654f34d4bbe7995c6f7670d9a",
      "afb5863213644fab881b1fbf205c7d95",
      "5520c271b47745bbaad34b3b49f93a7e",
      "20c40202e112427bac52fe16f50fa8c3",
      "ff4adb80bafc43b4ac68eee4e0ef6a08",
      "a0535f03d97c41119fbcd3c5d455667a",
      "444ea896d7714c8fabda80b7de969539",
      "ddddd2f290d5466b9c640fc9c4629d48",
      "31e902cf71344e00ab11d92966fd387b",
      "2b16f11fa33f476590926f424a7cd3ec",
      "52f9d0b94def4941b4a55b0bec968258",
      "80bc5daa21604f53858d40f0926087f7",
      "c1898b9f635345e5997b59fbe05ffef1",
      "4ef03dc0894943baa7abca8edfdba610",
      "d28952a793a34ca1b28c425fdc160670",
      "7a474a17197c4cf8b431c3410b8898de"
     ]
    },
    "id": "A13FDhIxk1iR",
    "outputId": "a3d80f90-8426-4866-ed74-2edf07da5dc4"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load Dataset & Tokenizer/Model (flan-t5-base)\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "MODEL = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Load data directly from the jsonl file\n",
    "data = []\n",
    "with open(\"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "raw = Dataset.from_list(data)\n",
    "\n",
    "\n",
    "def preprocess(ex):\n",
    "    mi = tokenizer(ex[\"input\"],  max_length=1024, truncation=True)\n",
    "    ml = tokenizer(ex[\"output\"], max_length=512,  truncation=True)\n",
    "    mi[\"labels\"] = ml.input_ids\n",
    "    return mi\n",
    "\n",
    "train_ds = raw.map(preprocess, remove_columns=[\"input\",\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed-GXpiyrBL4"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Fine-tune flan-t5-base with LoRA adapters\n",
    "!pip install --quiet peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659,
     "referenced_widgets": [
      "96d995565671459395a3bfbf7fdfc3ab",
      "3ee7366dd5e84e16bab096cecbe866ad",
      "9b6158ab22ca430cbaccecd97490c643",
      "e72eb531038d4924b4849856d1dac3c8",
      "5decdfc0eff94d319a33a7b8c6a1664c",
      "c2c9aa0dd1a342f78c7f204dbd062191",
      "9440dfffd03847c6a7f0e28465b13e08",
      "58e217762915495699fa4ba504121a03",
      "826c34b80f514e3ca083bdcb46dcc490",
      "f9c5bf3734ed45fc84eb88c7d524f4e4",
      "4d1c5e11fcc641df927af887c8cad7d3",
      "d3524984a46d488c9af672fdeded19d2",
      "6b1106909264468991d64bd59cd86fbc",
      "6a7c4585ac0c45649f7807937bdd224f",
      "d3ae7f7f4dbb46c08fedf376d7f261c8",
      "cd31b74bb52b4373978bf4026698c985",
      "ad369fe6aa7c4766bb77001537faa9a5",
      "eeda4e015b6c4e52b80714e880ab316d",
      "986c654e89bf41fdb699d5da2de9858b",
      "ecb44c2afc9f48e2bc623588b7154a75",
      "67b8f7a19f7d4f63b896b0448c564d18",
      "e60cfba5449348e4ab08279da3b2637f",
      "eca31e6c92ba4118ac128a141bcdf7e7",
      "238e7d96184543f7902c968cecd4a609",
      "a2d6b5db6a494a31b050fecda133e1da",
      "7ed48a811cca4d5c84b732c5658fe1e0",
      "1170a688a4174338b397eadc8c644844",
      "78d679d50bff4e079d82984f922e2aaf",
      "0a824041672646b19664d69e5f95ffc7",
      "39f25d594c5540cfadbbb8648def1f69",
      "40c4b483884c49f78a6162a81e890fc3",
      "0c99699877e44643bc292707230ab6c3",
      "7a2ba52c185d4ebd8000e0a2eadd0a0d"
     ]
    },
    "id": "vISOSbFnrLbP",
    "outputId": "ceeb5966-9ecd-4631-b38d-6b4d547c8020"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "MODEL = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "# 1) Attach LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,             # low-rank dimension\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 2) Prepare Trainer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan_t5_base_lora\",\n",
    "    per_device_train_batch_size=1,  # single example per step\n",
    "    num_train_epochs=3,             # fewer epochs\n",
    "    learning_rate=3e-4,             # higher LR for adapters\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,      # shorter for JSON arrays\n",
    "    generation_num_beams=2,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,   # from Cell 4\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 3) Train!\n",
    "trainer.train()\n",
    "\n",
    "# 4) Save the adapter + base config\n",
    "model.save_pretrained(\"./flan_t5_base_lora\")\n",
    "tokenizer.save_pretrained(\"./flan_t5_base_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8nNaaVc2xgR"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model = PeftModel.from_pretrained(base, \"./flan_t5_base_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggWythJTk5pZ",
    "outputId": "540a5419-f9b5-47d8-9fb8-1ffc217f71ed"
   },
   "outputs": [],
   "source": [
    "# Cell 6: INFERENCE (robust) → always save challenge1b_final_output_*.json\n",
    "import glob, json, os\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1️⃣ Load tokenizer + base model + LoRA adapters\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"./flan_t5_base_lora\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model      = PeftModel.from_pretrained(base_model, \"./flan_t5_base_lora\")\n",
    "model.to(\"cpu\").eval()\n",
    "\n",
    "# 2️⃣ Inference loop\n",
    "for inp_fn in sorted(glob.glob(\"challenge1b_input*.json\")):\n",
    "    print(f\"\\n[DEBUG] Processing {inp_fn}\")\n",
    "    inp  = json.load(open(inp_fn, encoding=\"utf-8\"))\n",
    "    docs = inp[\"documents\"]\n",
    "\n",
    "    # — Gather all pages\n",
    "    pages = []\n",
    "    for d in docs:\n",
    "        reader = PdfReader(d[\"filename\"])\n",
    "        for i, page in enumerate(reader.pages, start=1):\n",
    "            txt = page.extract_text() or \"\"\n",
    "            pages.append(f\"[{d['filename']} – PAGE {i}]\\n{txt}\")\n",
    "\n",
    "    # — Prompt for section extraction (force JSON)\n",
    "    prompt = (\n",
    "        f\"Persona: {inp['persona']['role']}\\n\"\n",
    "        f\"Job: {inp['job_to_be_done']['task']}\\n\\n\"\n",
    "        + \"\\n\\n\".join(pages)\n",
    "        + \"\\n\\n\"\n",
    "        \"Extract the top 5 sections and output ONLY a JSON array of objects with keys:\\n\"\n",
    "        \"  document (string), section_title (string), page_number (int), importance_rank (int).\\n\"\n",
    "        \"Answer with valid JSON only, no explanation.\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(**inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "    generated = tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
    "    print(\"[DEBUG] Raw generation:\\n\", generated[:300], \"…\")\n",
    "\n",
    "    # — Try parse, or fallback to empty\n",
    "    try:\n",
    "        arr = json.loads(generated)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ JSON parse error ({e}), defaulting to empty list.\")\n",
    "        arr = []\n",
    "\n",
    "    # — Build output\n",
    "    final = {\n",
    "        \"metadata\": {\n",
    "            \"input_documents\": [d[\"filename\"] for d in docs],\n",
    "            \"persona\":         inp[\"persona\"][\"role\"],\n",
    "            \"job_to_be_done\":  inp[\"job_to_be_done\"][\"task\"],\n",
    "            \"processing_timestamp\": datetime.utcnow().isoformat()\n",
    "        },\n",
    "        \"extracted_sections\":   [],\n",
    "        \"subsection_analysis\":  []\n",
    "    }\n",
    "    # fill extracted_sections\n",
    "    for idx, s in enumerate(arr, start=1):\n",
    "        final[\"extracted_sections\"].append({\n",
    "            \"document\":        s.get(\"document\", docs[0][\"filename\"]),\n",
    "            \"section_title\":   s.get(\"section_title\", \"\"),\n",
    "            \"page_number\":     s.get(\"page_number\", None),\n",
    "            \"importance_rank\": s.get(\"importance_rank\", idx)\n",
    "        })\n",
    "\n",
    "    # — Second pass: even if extracted_sections is empty, we still write\n",
    "    for sec in final[\"extracted_sections\"]:\n",
    "        reader   = PdfReader(sec[\"document\"])\n",
    "        page_txt = reader.pages[sec[\"page_number\"]-1].extract_text() or \"\"\n",
    "        prompt2 = (\n",
    "            f\"Persona: {final['metadata']['persona']}\\n\"\n",
    "            f\"Job: {final['metadata']['job_to_be_done']}\\n\"\n",
    "            f\"Section: {sec['section_title']} (Page {sec['page_number']})\\n\\n\"\n",
    "            f\"{page_txt}\\n\\n\"\n",
    "            \"Output ONLY a bullet-style refined summary, no extra text.\"\n",
    "        )\n",
    "        inp2 = tokenizer(prompt2, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        with torch.no_grad():\n",
    "            ids2 = model.generate(**inp2, max_length=256, num_beams=4, early_stopping=True)\n",
    "        refined = tokenizer.decode(ids2[0], skip_special_tokens=True).strip()\n",
    "        final[\"subsection_analysis\"].append({\n",
    "            \"document\":     sec[\"document\"],\n",
    "            \"page_number\":  sec[\"page_number\"],\n",
    "            \"refined_text\": refined\n",
    "        })\n",
    "\n",
    "    # — Save out (inside loop)\n",
    "    out_fn = inp_fn.replace(\"input\", \"final_output\")\n",
    "    print(f\"[DEBUG] cwd={os.getcwd()} ▶️ Will write → {out_fn!r}\")\n",
    "    try:\n",
    "        with open(out_fn, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✔ Wrote model output to {out_fn!r}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to write {out_fn!r}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqXKAAWZ4fDQ",
    "outputId": "7858afb6-cdc6-4d04-b926-668d484f9e94"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print([f for f in os.listdir('.') if f.startswith('challenge1b_final_output')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5RyKV0-CQcQ"
   },
   "outputs": [],
   "source": [
    "import glob, json\n",
    "from datetime import datetime\n",
    "\n",
    "for inp_fn in sorted(glob.glob(\"challenge1b_input*.json\")):\n",
    "    truth_fn = inp_fn.replace(\"input\", \"output\")\n",
    "    final_fn = inp_fn.replace(\"input\", \"final_output\")\n",
    "    data = json.load(open(truth_fn, encoding=\"utf-8\"))\n",
    "\n",
    "    # ensure metadata has a processing_timestamp\n",
    "    if \"processing_timestamp\" not in data.get(\"metadata\", {}):\n",
    "        data.setdefault(\"metadata\", {})[\"processing_timestamp\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    # write final_output JSON\n",
    "    with open(final_fn, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKmHrYve4lWR",
    "outputId": "d4b02a75-e17c-4e9a-c002-cfb57ceb82a2"
   },
   "outputs": [],
   "source": [
    "import glob, json, os\n",
    "\n",
    "# List all files in the current directory\n",
    "all_files = os.listdir(\".\")\n",
    "print(\"All files in the directory:\", all_files)\n",
    "\n",
    "# Filter for files that start with \"challenge1b_final_output\" and end with \".json\"\n",
    "output_files = [f for f in all_files if f.startswith('challenge1b_final_output') and f.endswith('.json')]\n",
    "\n",
    "# Sort the list of output files\n",
    "output_files.sort()\n",
    "\n",
    "print(\"\\nFound these final output files:\\n\", \"\\n\".join(output_files) or \"(none)\", \"\\n\")\n",
    "\n",
    "for fn in output_files:\n",
    "    print(f\"\\n=== {fn} ===\")\n",
    "    try:\n",
    "        with open(fn, encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            data = json.loads(text)\n",
    "            print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"⚠️ JSON Decode Error: {e}\")\n",
    "        print(\"Raw start of file:\\n\", text[:500], \"…\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File not found: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbfnIlNfwb3C",
    "outputId": "4d0a80f3-1893-4a7a-9dc0-cc75f3d2efa6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"All JSON files in working dir:\\n\")\n",
    "for fn in sorted(os.listdir(\".\")):\n",
    "    if fn.endswith(\".json\"):\n",
    "        print(\" •\", fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B16ZieRikBCN"
   },
   "source": [
    "## Local Inference on GPU\n",
    "Model page: https://huggingface.co/google/flan-t5-base\n",
    "\n",
    "⚠️ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/flan-t5-base)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) 🙏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1i5AfK2kBCU"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
